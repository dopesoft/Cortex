"""
Metrics and scoring utilities for Jean Memory evaluations
Provides standardized scoring functions across all evaluation types
"""

import re
import logging
from typing import Any, Dict, List, Set, Tuple, Union
from dataclasses import dataclass
import asyncio
import json

logger = logging.getLogger(__name__)

@dataclass
class ContextQualityScore:
    """Detailed context quality scoring breakdown"""
    relevance_score: float    # 0-100: How relevant to the user query
    completeness_score: float # 0-100: Includes necessary background
    personalization_score: float # 0-100: Reflects user's specific context
    noise_penalty: float      # 0-100: Penalty for irrelevant information
    overall_score: float      # Weighted combination
    
    def to_dict(self) -> Dict[str, float]:
        return {
            'relevance': self.relevance_score,
            'completeness': self.completeness_score,
            'personalization': self.personalization_score,
            'noise_penalty': self.noise_penalty,
            'overall': self.overall_score
        }

@dataclass
class PerformanceScore:
    """Performance evaluation scoring"""
    response_time: float      # Actual response time in seconds
    target_time: float        # Target response time
    timeout_occurred: bool    # Whether operation timed out
    graceful_degradation: bool # Whether system degraded gracefully
    throughput_score: float   # Concurrent request handling
    overall_score: float
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'response_time': self.response_time,
            'target_time': self.target_time,
            'timeout_occurred': self.timeout_occurred,
            'graceful_degradation': self.graceful_degradation,
            'throughput': self.throughput_score,
            'overall': self.overall_score
        }

class ContextQualityEvaluator:
    """Evaluates the quality of context generated by the orchestrator"""
    
    def __init__(self):
        self.relevance_keywords = {}  # Will be populated based on query type
        
    def evaluate_context_quality(self, context: str, user_query: str, 
                                expected_elements: List[str] = None,
                                user_profile: Dict[str, Any] = None) -> ContextQualityScore:
        """
        Evaluate context quality across multiple dimensions
        """
        # 1. Relevance Score (40% weight)
        relevance_score = self._calculate_relevance_score(context, user_query)
        
        # 2. Completeness Score (30% weight)  
        completeness_score = self._calculate_completeness_score(context, expected_elements or [])
        
        # 3. Personalization Score (20% weight)
        personalization_score = self._calculate_personalization_score(context, user_profile or {})
        
        # 4. Noise Penalty (10% weight)
        noise_penalty = self._calculate_noise_penalty(context, user_query)
        
        # Calculate weighted overall score
        overall_score = (
            relevance_score * 0.4 +
            completeness_score * 0.3 +
            personalization_score * 0.2 -
            noise_penalty * 0.1
        )
        overall_score = max(0, min(100, overall_score))  # Clamp to 0-100
        
        return ContextQualityScore(
            relevance_score=relevance_score,
            completeness_score=completeness_score,
            personalization_score=personalization_score,
            noise_penalty=noise_penalty,
            overall_score=overall_score
        )
    
    def _calculate_relevance_score(self, context: str, user_query: str) -> float:
        """Calculate how relevant context is to the user query"""
        if not context or not user_query:
            return 0.0
            
        # Extract key terms from query
        query_terms = self._extract_key_terms(user_query.lower())
        context_lower = context.lower()
        
        # Calculate term overlap
        matching_terms = sum(1 for term in query_terms if term in context_lower)
        term_coverage = matching_terms / len(query_terms) if query_terms else 0
        
        # Check for semantic relevance indicators
        semantic_score = self._check_semantic_relevance(context, user_query)
        
        # Combine scores (70% term coverage, 30% semantic)
        relevance_score = (term_coverage * 0.7 + semantic_score * 0.3) * 100
        
        return min(100, relevance_score)
    
    def _calculate_completeness_score(self, context: str, expected_elements: List[str]) -> float:
        """Calculate how complete the context is based on expected elements"""
        if not expected_elements:
            # If no expected elements specified, check for general completeness indicators
            return self._check_general_completeness(context)
        
        context_lower = context.lower()
        found_elements = sum(1 for element in expected_elements 
                           if element.lower() in context_lower)
        
        completeness_ratio = found_elements / len(expected_elements)
        return completeness_ratio * 100
    
    def _calculate_personalization_score(self, context: str, user_profile: Dict[str, Any]) -> float:
        """Calculate how personalized the context is to the specific user"""
        if not user_profile:
            # Check for general personalization indicators
            return self._check_general_personalization(context)
        
        personalization_indicators = 0
        total_checks = 0
        
        # Check for user-specific information
        context_lower = context.lower()
        
        # Check profession/role
        if 'profession' in user_profile:
            total_checks += 1
            if user_profile['profession'].lower() in context_lower:
                personalization_indicators += 1
        
        # Check preferences
        if 'preferences' in user_profile:
            for pref in user_profile['preferences']:
                total_checks += 1
                if pref.lower() in context_lower:
                    personalization_indicators += 1
        
        # Check interests
        if 'interests' in user_profile:
            for interest in user_profile['interests']:
                total_checks += 1
                if interest.lower() in context_lower:
                    personalization_indicators += 1
        
        if total_checks == 0:
            return self._check_general_personalization(context)
        
        return (personalization_indicators / total_checks) * 100
    
    def _calculate_noise_penalty(self, context: str, user_query: str) -> float:
        """Calculate penalty for irrelevant or noisy information"""
        if not context:
            return 0.0
        
        # Check for common noise indicators
        noise_indicators = [
            'system directive',
            'generated with',
            'as an ai',
            'i don\'t have',
            'i cannot',
            'error',
            'failed to',
            'timeout'
        ]
        
        context_lower = context.lower()
        noise_count = sum(1 for indicator in noise_indicators if indicator in context_lower)
        
        # Check for repetitive information
        sentences = context.split('.')
        unique_sentences = set(s.strip().lower() for s in sentences if s.strip())
        repetition_ratio = 1 - (len(unique_sentences) / len(sentences)) if sentences else 0
        
        # Calculate total noise penalty
        noise_penalty = (noise_count * 10) + (repetition_ratio * 30)
        return min(100, noise_penalty)
    
    def _extract_key_terms(self, text: str) -> List[str]:
        """Extract key terms from text for relevance calculation"""
        # Remove common stop words and extract meaningful terms
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'about', 'what', 'how', 'when', 'where', 'why', 'who'}
        
        # Extract words (alphanumeric sequences)
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        key_terms = [word for word in words if word not in stop_words and len(word) > 2]
        
        return key_terms
    
    def _check_semantic_relevance(self, context: str, query: str) -> float:
        """Check for semantic relevance beyond keyword matching"""
        # Simple heuristic-based semantic relevance checking
        # In a production system, this could use embeddings or more sophisticated NLP
        
        query_lower = query.lower()
        context_lower = context.lower()
        
        # Check for question-answer patterns
        if '?' in query:
            if any(indicator in context_lower for indicator in ['user', 'you', 'your', 'preferences', 'interests']):
                return 0.8
        
        # Check for technical queries
        technical_terms = ['code', 'programming', 'software', 'python', 'javascript', 'react', 'ai']
        if any(term in query_lower for term in technical_terms):
            if any(term in context_lower for term in technical_terms):
                return 0.9
        
        return 0.5  # Default moderate relevance
    
    def _check_general_completeness(self, context: str) -> float:
        """Check for general completeness indicators when no specific elements expected"""
        if not context:
            return 0.0
        
        completeness_indicators = [
            'user', 'preferences', 'interests', 'background', 'experience',
            'current', 'working', 'projects', 'goals', 'values'
        ]
        
        context_lower = context.lower()
        found_indicators = sum(1 for indicator in completeness_indicators 
                             if indicator in context_lower)
        
        # Also check length - very short context is likely incomplete
        length_score = min(1.0, len(context) / 200)  # 200 chars = reasonable minimum
        
        indicator_score = found_indicators / len(completeness_indicators)
        
        return (indicator_score * 0.7 + length_score * 0.3) * 100
    
    def _check_general_personalization(self, context: str) -> float:
        """Check for general personalization when no user profile available"""
        if not context:
            return 0.0
        
        personalization_indicators = [
            'user is', 'user has', 'user prefers', 'user likes', 'user works',
            'your', 'you are', 'you have', 'you prefer', 'you like'
        ]
        
        context_lower = context.lower()
        found_indicators = sum(1 for indicator in personalization_indicators 
                             if indicator in context_lower)
        
        # Check for specific vs generic language
        specific_indicators = ['specific', 'particular', 'unique', 'personal', 'individual']
        generic_indicators = ['general', 'common', 'typical', 'standard', 'default']
        
        specific_count = sum(1 for indicator in specific_indicators if indicator in context_lower)
        generic_count = sum(1 for indicator in generic_indicators if indicator in context_lower)
        
        specificity_score = (specific_count - generic_count) / max(1, specific_count + generic_count)
        specificity_score = max(0, specificity_score)  # Ensure non-negative
        
        indicator_score = found_indicators / len(personalization_indicators)
        
        return (indicator_score * 0.6 + specificity_score * 0.4) * 100

class MemoryTriageEvaluator:
    """Evaluates memory triage accuracy (remember vs skip decisions)"""
    
    def __init__(self):
        self.memorable_indicators = [
            'i am', 'i\'m', 'my name is', 'i work', 'i live', 'i like', 'i prefer',
            'i studied', 'i graduated', 'my goal', 'my project', 'i\'m working on',
            'remember that', 'important to note', 'keep in mind'
        ]
        
        self.skip_indicators = [
            'what is', 'how do', 'can you', 'help me', 'explain', 'tell me about',
            'what time', 'when', 'where', 'why', 'thanks', 'thank you', 'got it',
            'okay', 'yes', 'no', 'maybe'
        ]
    
    def evaluate_triage_decision(self, message: str, decision: str, 
                                expected_decision: str = None) -> Dict[str, Any]:
        """
        Evaluate whether the memory triage decision was correct
        
        Args:
            message: The user message being evaluated
            decision: Actual decision made ('REMEMBER' or 'SKIP')  
            expected_decision: Expected decision for comparison
            
        Returns:
            Dictionary with evaluation results
        """
        # If we have expected decision, use it for accuracy
        if expected_decision:
            accuracy = 1.0 if decision.upper() == expected_decision.upper() else 0.0
        else:
            # Use heuristic evaluation
            accuracy = self._evaluate_triage_heuristic(message, decision)
        
        # Evaluate confidence based on message characteristics
        confidence = self._calculate_triage_confidence(message, decision)
        
        return {
            'accuracy': accuracy,
            'confidence': confidence,
            'decision': decision,
            'expected': expected_decision,
            'message_analysis': self._analyze_message_characteristics(message)
        }
    
    def _evaluate_triage_heuristic(self, message: str, decision: str) -> float:
        """Evaluate triage decision using heuristic rules"""
        message_lower = message.lower()
        
        memorable_score = sum(1 for indicator in self.memorable_indicators 
                            if indicator in message_lower)
        skip_score = sum(1 for indicator in self.skip_indicators 
                       if indicator in message_lower)
        
        # Determine expected decision based on indicators
        if memorable_score > skip_score:
            expected = 'REMEMBER'
        elif skip_score > memorable_score:
            expected = 'SKIP'  
        else:
            # Tie-breaker: check message length and personal pronouns
            if len(message) > 30 and ('i ' in message_lower or 'my ' in message_lower):
                expected = 'REMEMBER'
            else:
                expected = 'SKIP'
        
        return 1.0 if decision.upper() == expected else 0.0
    
    def _calculate_triage_confidence(self, message: str, decision: str) -> float:
        """Calculate confidence in the triage decision"""
        message_lower = message.lower()
        
        # Strong remember indicators
        strong_remember = ['i am a', 'i work as', 'my name is', 'i live in', 'remember that']
        # Strong skip indicators  
        strong_skip = ['what is', 'how do i', 'can you help', 'explain to me']
        
        strong_remember_count = sum(1 for indicator in strong_remember if indicator in message_lower)
        strong_skip_count = sum(1 for indicator in strong_skip if indicator in message_lower)
        
        if decision.upper() == 'REMEMBER':
            confidence = min(1.0, 0.5 + strong_remember_count * 0.25)
        else:  # SKIP
            confidence = min(1.0, 0.5 + strong_skip_count * 0.25)
        
        return confidence
    
    def _analyze_message_characteristics(self, message: str) -> Dict[str, Any]:
        """Analyze characteristics of the message for debugging"""
        message_lower = message.lower()
        
        return {
            'length': len(message),
            'word_count': len(message.split()),
            'has_personal_pronouns': any(pronoun in message_lower for pronoun in ['i ', 'my ', 'me ']),
            'has_questions': '?' in message,
            'has_factual_info': any(indicator in message_lower for indicator in ['i am', 'i work', 'i live']),
            'is_command': any(command in message_lower for command in ['help', 'explain', 'tell me', 'show me']),
            'memorable_indicators_count': sum(1 for indicator in self.memorable_indicators if indicator in message_lower),
            'skip_indicators_count': sum(1 for indicator in self.skip_indicators if indicator in message_lower)
        }

class PerformanceEvaluator:
    """Evaluates performance metrics like response time and resilience"""
    
    def __init__(self):
        self.fast_path_target = 3.0  # seconds
        self.deep_analysis_target = 60.0  # seconds
    
    def evaluate_performance(self, response_time: float, target_time: float,
                           timeout_occurred: bool = False,
                           graceful_degradation: bool = True,
                           concurrent_requests: int = 1) -> PerformanceScore:
        """
        Evaluate performance across multiple dimensions
        """
        # Response time score (0-100)
        if response_time <= target_time:
            time_score = 100.0
        elif response_time <= target_time * 2:
            # Linear degradation up to 2x target time
            time_score = 100.0 - ((response_time - target_time) / target_time) * 50
        else:
            # Severe penalty for > 2x target time
            time_score = max(0, 50 - ((response_time - target_time * 2) / target_time) * 10)
        
        # Timeout penalty
        if timeout_occurred:
            time_score *= 0.1  # 90% penalty for timeouts
        
        # Graceful degradation bonus
        degradation_score = 100.0 if graceful_degradation else 0.0
        
        # Throughput score (simplified - would need actual throughput data)
        throughput_score = max(0, 100 - (concurrent_requests - 1) * 10)  # Penalty for high concurrency
        
        # Overall performance score (weighted combination)
        overall_score = (
            time_score * 0.5 +           # 50% response time
            degradation_score * 0.3 +    # 30% graceful degradation  
            throughput_score * 0.2       # 20% throughput
        )
        
        return PerformanceScore(
            response_time=response_time,
            target_time=target_time,
            timeout_occurred=timeout_occurred,
            graceful_degradation=graceful_degradation,
            throughput_score=throughput_score,
            overall_score=overall_score
        )
    
    def evaluate_fast_path_performance(self, response_time: float, 
                                     context_provided: bool = True) -> Dict[str, Any]:
        """Evaluate fast path specific performance requirements"""
        target_met = response_time <= self.fast_path_target
        
        # Fast path should always provide some response
        availability_score = 100.0 if context_provided else 0.0
        
        # Calculate performance score
        perf_score = self.evaluate_performance(response_time, self.fast_path_target)
        
        return {
            'target_met': target_met,
            'response_time': response_time,
            'target_time': self.fast_path_target,
            'availability_score': availability_score,
            'performance_score': perf_score.to_dict(),
            'fast_path_compliant': target_met and context_provided
        }

def calculate_overall_system_score(context_scores: List[ContextQualityScore],
                                 triage_results: List[Dict[str, Any]],
                                 performance_scores: List[PerformanceScore]) -> Dict[str, float]:
    """Calculate overall system evaluation score"""
    
    # Average context quality
    avg_context_score = sum(cs.overall_score for cs in context_scores) / len(context_scores) if context_scores else 0
    
    # Average triage accuracy  
    avg_triage_accuracy = sum(tr['accuracy'] for tr in triage_results) / len(triage_results) * 100 if triage_results else 0
    
    # Average performance score
    avg_performance_score = sum(ps.overall_score for ps in performance_scores) / len(performance_scores) if performance_scores else 0
    
    # Weighted overall score based on V2 testing strategy priorities
    overall_score = (
        avg_context_score * 0.4 +      # 40% context quality
        avg_performance_score * 0.35 +  # 35% performance  
        avg_triage_accuracy * 0.25      # 25% triage accuracy
    )
    
    return {
        'overall_score': overall_score,
        'context_quality': avg_context_score,
        'triage_accuracy': avg_triage_accuracy,
        'performance': avg_performance_score,
        'breakdown': {
            'context_tests': len(context_scores),
            'triage_tests': len(triage_results), 
            'performance_tests': len(performance_scores)
        }
    }